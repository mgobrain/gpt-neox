{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "super_sweep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNGbC9-bMdjw"
      },
      "source": [
        "import subprocess\n",
        "import wandb\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjOa3RyMMie5"
      },
      "source": [
        "This notebook does a WandB hyperparamter sweep by writing a `dummy.yml` file for use with the `pretrain_gpt2.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1DhNKAPM228"
      },
      "source": [
        "def sweep_to_yml(run_config,dummy_yml):\n",
        "  with open(dummy_yml, 'r') as f:\n",
        "    conf = yaml.load(f,Loader=yaml.Loader)\n",
        "  for k in ['num-layers', 'hidden-size', 'num-attention-heads']:\n",
        "    conf[k] = run_config[k]\n",
        "  conf['optimizer']['params']['lr'] = run_config['lr']\n",
        "  with open(dummy_yml, 'w') as f:\n",
        "    yaml.dump(conf, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zFKDqw2urSu"
      },
      "source": [
        "sweep_config = {\n",
        "    'name': 'Scaling Laws for Neural Language Models sweep',\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'valid_set': {\n",
        "          'values': [\n",
        "                    # This will be a list of strings\n",
        "          ]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "param_dict = {\n",
        "  # these are the ranges to sweep over\n",
        "  # ballpark numbers from Figure 5\n",
        "  'exponent': [exponent for exponent in range(10,22)],\n",
        "  'ar': [round(10**x) for x in np.linspace(1,2.5,3)],\n",
        "  'attn_dim': [round(10**x) for x in np.linspace(1.5,2.5,3)],\n",
        "}\n",
        "\n",
        "for exponent in param_dict['exponent']:\n",
        "  N = np.exp(exponent)\n",
        "  # add LR according equation D.1 from Kaplan et. al\n",
        "  # \"Scaling Laws for Neural Language Models\"\n",
        "  lr = 0.003239 + (-0.0001395)*np.log(N)\n",
        "  for ar in param_dict['ar']:\n",
        "    # substitute for n_layer, solve for d_model\n",
        "    d_model = (N*ar/12)**(1/3)\n",
        "    # calculate n_layer\n",
        "    n_layer = N/12/(d_model**2)\n",
        "    if n_layer < 1:\n",
        "      # don't clip n_layer\n",
        "      break\n",
        "    for attn_dim in param_dict['attn_dim']:\n",
        "      # add n_head per attn_dim\n",
        "      n_head = d_model/attn_dim\n",
        "      if n_head < 1:\n",
        "        # don't clip n_head\n",
        "        break\n",
        "      # add this combination as a string to sweep_config\n",
        "      sweep_config['parameters']['valid_set']['values'].append(\n",
        "          ','.join(\n",
        "              [str(round(x)) for x in \\\n",
        "               [exponent,n_layer, d_model, n_head]] + \\\n",
        "               [str(float(lr))])\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBU0k06Du4cI"
      },
      "source": [
        "# test run\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "\n",
        "def train():\n",
        "  run = wandb.init()\n",
        "  variables = {k:v for k,v in zip(\n",
        "        # these are from neox_arguments.md\n",
        "        ['exponent',\n",
        "         'num-layers', # \"n_layers\" (GPT)\n",
        "         'hidden-size', # \"d_model\" (GPT)\n",
        "         'num-attention-heads', # \"n_heads\" (GPT)\n",
        "         'lr' # \"learning_rate\" (GPT)\n",
        "         ],\n",
        "        [float(x) for x in run.config.valid_set.split(',')]\n",
        "    )}\n",
        "  print(variables)\n",
        "  # write the run config to the dummy yaml\n",
        "  sweep_to_yml(variables, 'configs/dummy.yml')\n",
        "  # execute the actual pretrain process\n",
        "  subprocess.run('python deepy.py pretrain_gpt2.py -d configs dummy.yml local_setup.yml'.split(' '), capture_output=True)\n",
        "  run.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "agent = wandb.agent(sweep_id=sweep_id, function=train)\n",
        "agent.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}