{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "super_sweep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNGbC9-bMdjw"
      },
      "source": [
        "import subprocess\n",
        "import wandb\n",
        "import yaml\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjOa3RyMMie5"
      },
      "source": [
        "This notebook does a WandB hyperparamter sweep by writing a `dummy.yml` file for use with the `pretrain_gpt2.py` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdt1OhYj_mcb"
      },
      "source": [
        "sweep_config = {\n",
        "    'name': 'Scaling Laws for Neural Language Models sweep',\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        # top-level params can be passed as they are\n",
        "        'warmup': {\n",
        "            'values': [\n",
        "                       # multiple values will be swept over\n",
        "                       # in grid fashion\n",
        "                       0.01\n",
        "                       ]\n",
        "        },\n",
        "        # params that are constant can be changed in template\n",
        "        # or set to single values\n",
        "        'steps_per_print': {\n",
        "            'values': [\n",
        "                       10\n",
        "            ]\n",
        "        },\n",
        "        # this is a placeholder for calculated values below\n",
        "        'valid_set': {\n",
        "          'values': [\n",
        "                    # This will be a list of lists\n",
        "                     # one for each valid scaling law\n",
        "                     # sweep configuration\n",
        "          ]\n",
        "        },\n",
        "        'opt_configs': {\n",
        "            'values': [\n",
        "                       #Placeholder for list of lists,\n",
        "                       #one for each valid optimizer\n",
        "                       #sweep configuration\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "param_dict = {\n",
        "  # these are the ranges to sweep over\n",
        "  # during the scaling laws sweep\n",
        "  # ballpark numbers from Figure 5\n",
        "  'exponent': [exponent for exponent in range(10,22)],\n",
        "  'ar': [round(10**x) for x in np.linspace(1,2.5,3)],\n",
        "  'attn_dim': [round(10**x) for x in np.linspace(1.5,2.5,3)],\n",
        "  'opts': {\n",
        "      #These are optimizer-specific configs\n",
        "      #Each optimizer can take different args\n",
        "      #If type of optimizer does not match\n",
        "      #template yaml, must specify all args\n",
        "      #except LR (which is calculated in scaling laws)\n",
        "      'adam': {\n",
        "          'betas': [\n",
        "                    # list of lists expected\n",
        "                    # (as betas takes two values)\n",
        "                    [0.9, 0.999],\n",
        "                    [0.95, 0.95]\n",
        "                    ],\n",
        "          'eps': [1e-7]\n",
        "      },\n",
        "      'sm3': {\n",
        "          'momentum': [0.0, 0.01],\n",
        "          'beta': [0.0, 0.01],\n",
        "          'eps': [1e-30]\n",
        "      }\n",
        "\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1DhNKAPM228"
      },
      "source": [
        "def sweep_to_yml(run_config,template_yml,dummy_yml):\n",
        "  \"\"\"\n",
        "  Write the current run config to a dummy yaml\n",
        "  \"\"\"\n",
        "  with open(template_yml, 'r') as f:\n",
        "    conf = yaml.load(f,Loader=yaml.Loader)\n",
        "  for k in run_config.keys():\n",
        "    if k == 'optimizer':\n",
        "      # need to overwrite template config if optimizer differs\n",
        "      if run_config['optimizer']['type'] != conf['optimizer']['type']:\n",
        "        conf['optimizer'] = run_config['optimizer']\n",
        "      else:\n",
        "        for c in run_config['optimizer']['params'].keys():\n",
        "          conf['optimizer']['params'][c] = run_config['optimizer']['params'][c]\n",
        "    else:\n",
        "      conf[k] = run_config[k]\n",
        "  with open(dummy_yml, 'w') as f:\n",
        "    yaml.dump(conf, f)\n",
        "\n",
        "def grid(input,depth):\n",
        "  \"\"\"\n",
        "  Find all combinations of the input elements\n",
        "  Depth is for recursion\n",
        "  \"\"\"\n",
        "  if depth == len(input)-1:\n",
        "    return input[depth]\n",
        "  else:\n",
        "    output = []\n",
        "    for i in input[depth]:\n",
        "      for j in grid(input,depth+1):\n",
        "        if type(i) == list:\n",
        "          output.append([i,j])\n",
        "        else:\n",
        "          if type(j) == list:\n",
        "            output.append([i]+j)\n",
        "          else:\n",
        "            output.append([i,j])\n",
        "    return output\n",
        "\n",
        "def make_row(input,label):\n",
        "  \"\"\"\n",
        "  Call grid to make a valid combination\n",
        "  of hparams\n",
        "  \"\"\"\n",
        "  d = input.copy()\n",
        "  raw = grid([*d.values()],0)\n",
        "  output = []\n",
        "  for r in raw:\n",
        "    row = [label,r,list(d.keys())]\n",
        "    output.append(row)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zFKDqw2urSu"
      },
      "source": [
        "for exponent in param_dict['exponent']:\n",
        "  N = np.exp(exponent)\n",
        "  # add LR according equation D.1 from Kaplan et. al\n",
        "  # \"Scaling Laws for Neural Language Models\"\n",
        "  lr = 0.003239 + (-0.0001395)*np.log(N)\n",
        "  for ar in param_dict['ar']:\n",
        "    # substitute for n_layer, solve for d_model\n",
        "    d_model = (N*ar/12)**(1/3)\n",
        "    # calculate n_layer\n",
        "    n_layer = N/12/(d_model**2)\n",
        "    if n_layer < 1:\n",
        "      # don't clip n_layer\n",
        "      break\n",
        "    for attn_dim in param_dict['attn_dim']:\n",
        "      # add n_head per attn_dim\n",
        "      n_head = d_model/attn_dim\n",
        "      if n_head < 1:\n",
        "        # don't clip n_head\n",
        "        break\n",
        "      # add this combination as a string to sweep_config\n",
        "      sweep_config['parameters']['valid_set']['values'].append(\n",
        "              [round(x) for x in [n_layer, d_model, n_head]] + \\\n",
        "              [float(lr)]\n",
        "      )\n",
        "\n",
        "for opt_type in param_dict['opts'].keys():\n",
        "  for row in make_row(param_dict['opts'][opt_type],opt_type):\n",
        "    sweep_config['parameters']['opt_configs']['values'].append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBU0k06Du4cI"
      },
      "source": [
        "# test run\n",
        "def train():\n",
        "  run = wandb.init()\n",
        "  # convert wandb Config class to dictionary\n",
        "  config_dict = run.config.as_dict()\n",
        "  valid_set = {k:v for k,v in zip(\n",
        "        # these are from neox_arguments.md\n",
        "        [\n",
        "         'num-layers', # \"n_layers\" (GPT)\n",
        "         'hidden-size', # \"d_model\" (GPT)\n",
        "         'num-attention-heads', # \"n_heads\" (GPT)\n",
        "         'lr' # \"learning_rate\" (GPT)\n",
        "         ],\n",
        "        [float(x) for x in config_dict.pop('valid_set')]\n",
        "    )}\n",
        "  # initialize optimizer params\n",
        "  config_dict['optimizer'] = {\n",
        "      'params': {\n",
        "          # optimizer parameters will go here\n",
        "      }\n",
        "  }\n",
        "  # fill in optimizer params\n",
        "  for k in config_dict.keys():\n",
        "    if k == 'opt_configs':\n",
        "      config_dict['optimizer']['type'] = config_dict['opt_configs'][0]\n",
        "      opt_dict = {\n",
        "          k:v for k,v in zip(\n",
        "              config_dict['opt_configs'][-1],\n",
        "              config_dict['opt_configs'][1])\n",
        "          }\n",
        "      for c,v in opt_dict.items():\n",
        "        config_dict['optimizer']['params'][c] = v\n",
        "  # remove optimizer params from top-level\n",
        "  [config_dict.pop(k) for k in \\\n",
        "   [c for c in config_dict.keys() if c[:4]=='opt_']\n",
        "   ]\n",
        "  # set learning rate in optimizer parameters\n",
        "  config_dict['optimizer']['params']['lr'] = valid_set.pop('lr')\n",
        "  # transfer top-level params to run.config\n",
        "  for k in valid_set.keys():\n",
        "    config_dict[k] = valid_set[k]\n",
        "  # print the run config\n",
        "  print(config_dict)\n",
        "  # write the run config to the dummy yaml\n",
        "  sweep_to_yml(\n",
        "      config_dict,\n",
        "      template_yml='configs/small.yml',\n",
        "      dummy_yml='configs/dummy.yml'\n",
        "      )\n",
        "  # execute the actual pretrain process\n",
        "  cmd = subprocess.run('python deepy.py pretrain_gpt2.py -d configs dummy.yml local_setup.yml'.split(' '), capture_output=True)\n",
        "  print(cmd.stdout.decode())\n",
        "  run.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)\n",
        "agent = wandb.agent(sweep_id=sweep_id, function=train)\n",
        "agent.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}